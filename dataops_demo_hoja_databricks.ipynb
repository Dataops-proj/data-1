{"cells": [{"cell_type": "code", "source": ["import hvac \nimport pytz \nimport boto3 \nimport logging \nimport pyspark \nfrom pyspark.sql.types import * \nfrom pyspark.sql.functions import * \nfrom datetime import datetime \nfrom pyspark.sql import SparkSession \nimport base64\n\n#Configure logging\nindian_timezone=pytz.timezone(\"Asia/Kolkata\")\n# configure the logging module to include the Indian time stamp \nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\nformatter = logging.Formatter(\"%(asctime)s, %(levelname)s, %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S %Z\")\n#Time_converter_function\ndef converter(timestamp):\n\tutc_time = datetime.utcfromtimestamp(timestamp)\n\tlocal_time = pytz.utc.localize(utc_time).astimezone(indian_timezone)\n\treturn local_time.timetuple()\nformatter.converter = converter\n\nhandler = logging.StreamHandler()\nhandler.setFormatter(formatter)\nlogger.addHandler(handler)\n#logging_file\nfile_handler = logging.FileHandler(\"audit_logs.csv\")\nfile_handler.setFormatter(formatter)\nlogger.addHandler(file_handler)\n\nwith open(\"audit_logs.csv\", \"r+\") as f:\n\tcontent = f.read()\n\tf.seek(0, 0)\n\tf.write(\"TimeStamp, Log_level, Log_Message\\n\" + content)"], "metadata": {"application/vnd.databricks.v1+cell": {"showTitle": false, "cellMetadata": {}, "inputWidgets": {}, "title": "import modules"}}, "outputs": [], "execution_count": 0}, {"cell_type": "code", "source": ["\ntry:\n\tlogging.info('Starting data processing pipeline...')\n\n\tspark=SparkSession.builder.appName('DATA-OPS').getOrCreate()\n\tsc = spark.sparkContext\n\tlogging.info('Spark Context is created')\n\n\turl_dcp = base64.b64decode('aHR0cDovLzUyLjM0LjE0LjU2OjgyMDA=').decode('utf-8')\n\ttoken_dcp = base64.b64decode('cy5nWDVTNTdZaVhWczFPc0tUZ0VmSlhXUWQ=').decode('utf-8')\n\n\tclient = hvac.Client(url=url_dcp, token=token_dcp)\n\ts_s3_credentials = client.read('kv/data/data/s3_credentials')['data']['data']\n\taccess_key = s_s3_credentials.get('access_key')\n\tsecret_key = s_s3_credentials.get('secret_key')\n\taws_region = 'ap-south-1'\n\tlogging.info('AWS S3 credentials authenticated from Hvac Vault')\n\n\t#Configure Spark to use AWS S3 credentials\n\n\tsc._jsc.hadoopConfiguration().set('fs.s3a.access.key', access_key)\n\tsc._jsc.hadoopConfiguration().set('fs.s3a.secret.key', secret_key)\n\tsc._jsc.hadoopConfiguration().set('fs.s3a.endpoint', 's3.' + aws_region + '.amazonaws.com')\n\n\t#Read data from S3 bucket\n\tdf = spark.read.format('csv').options(header='True').load('s3://red-buckets/us-500.csv')\n\tlogging.info('The file us-500.csv loaded from S3 bucket successfully')\n\n\t#Get the number of rows\n\tnum_rows = df.count()\n\tlogging.info(f'Number of rows in the file: {num_rows}')\n\n\t# Get the number of columns\n\tnum_cols = len(df.schema.fields)\n\tlogging.info(f'Number of columns in the file: {num_cols}')"], "metadata": {"application/vnd.databricks.v1+cell": {"showTitle": false, "cellMetadata": {}, "inputWidgets": {}, "title": "Actions"}}, "outputs": [], "execution_count": 0}, {"cell_type": "code", "source": ["\n\n\n\t#Validation-notempty\n\tdf = df.filter(~col('first_name').isNull()).limit(100)\n\tdf = df.filter(~col('last_name').isNull()).limit(100)\n\n\t#Validation-custom\n\tif df.filter(df['company_name'].rlike('@')).count() > 0: \n\t\traise ValueError('Custom validation failed. Stopping processing.')  \n\telif df.filter(df['city'].rlike('@')).count() > 0: \n\t\traise ValueError('Custom validation failed. Stopping processing.')  \n\telif df.filter(df['address'].rlike('@')).count() > 0: \n\t\traise ValueError('Custom validation failed. Stopping processing.')  \n\n\t#Transformations\n\telse:\n\t\tdf = df.withColumn('first_name', df['first_name'].cast('string'))\n\t\tdf = df.withColumn('last_name', df['last_name'].cast('string'))\n\t\tdf = df.withColumn('company_name', df['company_name'].cast('string'))\n\t\tdf = df.withColumn('address', df['address'].cast('string'))\n\t\tdf = df.withColumn('city', df['city'].cast('string'))\n\t\tdf = df.withColumn('FULLNAME', concat(\"first_name\", \"last_name\"))"], "metadata": {"application/vnd.databricks.v1+cell": {"showTitle": false, "cellMetadata": {}, "inputWidgets": {}, "title": "Actions"}}, "outputs": [], "execution_count": 0}, {"cell_type": "code", "source": ["\n\n\tlogging.info('Data Transformation completed successfully')\n\n\t#writing the dataframe to s3 bucket\n\tdf.write.mode('overwrite').format('parquet').save('s3a://blue-buckets/mari/')\n\n\tlogging.info('Data written to S3 bucket successfully')\n\tlogging.info('Data processing pipeline completed.')\nexcept Exception as e:\n\tlogging.error('Error occurred during data processing: {}'.format(str(e)))\n#Move custom log file to S3 bucket \ns3 = boto3.client('s3', aws_access_key_id=s_s3_credentials.get('access_key'), aws_secret_access_key=s_s3_credentials.get('secret_key'),region_name=aws_region)\n\n# Upload custom log file to S3\ns3.upload_file('audit_logs.csv', 'blue-buckets', 'logs/audit_logs.csv')\nlogging.info('Custom log file saved to S3 successfully.')"], "metadata": {"application/vnd.databricks.v1+cell": {"showTitle": false, "cellMetadata": {}, "inputWidgets": {}, "title": "Actions"}}, "outputs": [], "execution_count": 0}], "metadata": {"application/vnd.databricks.v1+notebook": {"notebookName": "dataops_demo_hoja", "dashboards": [], "notebookMetadata": {"pythonIndentUnit": 4}, "language": "python", "widgets": {}}}, "nbformat": 4, "nbformat_minor": 0}